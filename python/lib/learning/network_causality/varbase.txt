"""Abstract base class for Vector AutoRegressive Models"""

from abc import ABCMeta, abstractmethod
from typing import Dict
from sklearn.datasets import make_regression
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from operator import itemgetter

class VarBase(metaclass=ABCMeta):
    """Abstract Base class representing the VAR model"""
    @abstractmethod
    def fit(self):
        pass

    @abstractmethod
    def predict(self):
        pass

    def simulate_var(self, n_iter = 1000, corr=0.8):
        n = n_iter
        cov_matrix = np.array([[1, 0.8], [0.8,1]])
        mean_vector = np.zeros(2)
        w = np.random.multivariate_normal(mean=mean_vector, cov=cov_matrix, size=n)
        x = np.zeros(n)
        y = np.zeros(n)
        wx = w[:,0]
        wy = w[:,1]
        x[0] = wx[0]
        y[0] = wy[0]
        for i in range(1, n):
            x[i] = 0.1 + 0.01 * x[i-1] + 0.3 * y[i-1] + wx[i]
            y[i] = 0.1 + 0.3 * x[i-1] + 0.01 * y[i-1] + wy[i]
        return (x,y)
    
    def make_polynomial(self, X: np.ndarray, fit_intercept=None) -> np.ndarray:
        if fit_intercept is None:
            bias = self.fit_intercept
        else:
            bias = fit_intercept 
        degree = self.degree
        pf = PolynomialFeatures(degree=degree, include_bias=bias)
        return pf.fit_transform(X)

    def reg_plot(self, X, y):
        plt.figure(figsize=(10,6))
        plt.scatter(X, y)
        # sort by design matrix -- needed for matplotlib
        sorted_values = iter(sorted(zip(X.flatten(), self.predictions), key=itemgetter(0)))
        X, pred = zip(*sorted_values)
        plt.plot(X, pred,'m-')
        plt.title("Regression Plot")