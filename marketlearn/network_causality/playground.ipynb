{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Econometric Measures of Connectedness and Risk in Financial Markets\n",
    "### The model \n",
    "- Let $1 + R_t = \\frac{P_t}{P_{t-1}}$ be the percentage return for asset i between times t-1 and t.   We define log return as $r_t = log(1 + R_t)$\n",
    "- Volatility $\\sigma_t^2 = Var(r_t^2 | F_{t-1}) = E[(r_t -  \\mu_t)^2 | F_{t-1}]$\n",
    "- Assumes Garch(1,1) model for volatility of asset returns.  \n",
    "- More specifically, for log returns $r_t$, let $a_t = r_t - \\mu_t$ be innovation at time t.  Then $a_t$ ~ Garch(1,1) if:\n",
    "    \n",
    "$a_t = \\sigma_t \\epsilon_t$  \n",
    "\n",
    "$\\sigma_t^2 = \\omega_0 + \\gamma_1 a_{t-1} + \\beta_1 \\sigma_{t-1}^2$\n",
    "\n",
    "where $\\epsilon_t$ is the error process, $\\epsilon_t$ ~ iid $N(0,1)$ and $(\\gamma_1 + \\beta_1) < 1$  \n",
    "### Note\n",
    "- The constraint is needed to ensure the variance process is finite\n",
    "\n",
    "### QuasiMaximum Likelihood Estimation\n",
    "- In order to estimate the parameters of the model, we are going to be maximizing the loglikelihood of the innovation series.  \n",
    "- Let $\\theta = (\\omega_0, \\gamma_1, \\beta_1)$ be the parameter we are trying to estimate\n",
    "- Let $L(\\theta | a_t, a_{t-1}, ..., a_0) = f(a_t, a_{t-1}, ..., a_0 | \\theta)$ be the likelihood function which is the joint density of interest \n",
    "- To derive the joint density, we use the law of total probability;\n",
    "  - $f(a_t, a_{t-1}, ..., a_0 | \\theta) = f(a_t | F_{t-1})f(a_{t-1} | F_{t-2}) ...f(a_1 | F_0)f(a_o | \\theta) = f(a_o | \\theta)\\prod_{k=1}^t f(a_k | F_{k-1})$\n",
    "\n",
    "    So...\n",
    "\n",
    "    $L(\\theta | a_t, a_{t-1}, ..., a_0) = f(a_o | \\theta)\\prod_{k=1}^t f(a_k | F_{k-1})$\n",
    "\n",
    "- __Note__: We are taking the maximum likelihood of past innovation in the series to the present time t\n",
    "\n",
    "- Taking the log of the likelihood, we obtain the following equation: \n",
    "  - $log(L(\\theta | a_t, a_{t-1}, ..., a_0)) = \\log{f(a_o | \\theta)} + \\sum_{k=1}^T \\log{f(a_k | F_{k-1})}$\n",
    "\n",
    "- In a large sample size, contribution from $\\log{f(a_o | \\theta)}$ is small and hence dropped\n",
    "- Since $\\epsilon_t$ ~ $N(0,1)$, then $a_t$ ~ $N(0, \\sigma_t^2)= \\frac{1}{\\sqrt{2\\pi\\sigma_t^2}}e^{-\\frac{a_t^2}{2\\sigma^2_t}}$\n",
    "  \n",
    "  So \n",
    "\n",
    "  $\\log{L(\\theta | a_t, a_{t-1}, ..., a_0)} = \\sum_{k=1}^t-\\frac{1}{2}\\log{2\\pi\\sigma_k^2} - \\frac{a_k^2}{2\\sigma^2_k}$\n",
    "\n",
    "- Factoring out the above equation, we get...\n",
    "\n",
    "  - $\\log{L(\\theta | a_t, a_{t-1}, ..., a_0)} = -(\\frac{t}{2}\\log{2\\pi} + \\sum_{k=1}^{t} \\frac{1}{2}\\log{\\sigma_k^2} + \\frac{a_k^2}{2\\sigma^2_k})$\n",
    "\n",
    "\n",
    "- Above is same as minimizing -$\\log{L(\\theta | a_t, a_{t-1}, ..., a_0)}:=QL(\\theta)$\n",
    "- Since the constant has no affect, we drop it, and get the desired Quasi MLE: \n",
    "\n",
    "$QL(\\theta) = \\frac{1}{2}\\sum_{k=1}^{t}\\log{\\sigma_k^2(\\theta)} + \\frac{a_k^2}{\\sigma^2_k(\\theta)}$\n",
    "\n",
    "### Principal Component Analysis\n",
    "- Authors use pca on a system of returns.  More specificially, let $\\sum_{R} = QDQ^T$ represent the spectral decomposition of \n",
    "covariance matrix of $R_t$ where each column is given by $R_i(t) = \\frac{R_i(t) - \\mu_i(t)}{\\sigma_i(t)}$\n",
    "- D is the eigen values of $\\sum_R$ and $Q$ is orthogonol eigen vectors\n",
    "- Matrix of principal components of $\\sum_R$ is then a Txp matrix P given by: $P = RQ$\n",
    "- Since $Q = col(q_k)_{k=1:n}$ then $RQ = col(Rq_k)_{k=1:n} = (Rq_1 | Rq_2 | ... | Rq_n)$ \n",
    "  - Hence $Rq_k = \\sum_{i=1}^n R_i q_k(i) = p_k$ the kth column of matrix P \n",
    "  - Hence kth principal component of $\\sum_R$ is defined as kth column of P and given by $p_k = R_1q_k(1) + R_2q_k(2) + ... + R_nq_k(n)$\n",
    "### Covariance of Principal Component \n",
    "- $\\sum_p = T^{-1}PP^T = T^{-1}(RQ)^T(RQ) = T^{-1}(Q^TR^TRQ) = Q^T \\sum_R Q = Q^T (QDQ^T)Q = D$; A diagonal whose entries are eigen values of $\\sum_R$\n",
    "- Variance of kth principal component is given by $\\lambda_k$\n",
    "- Total variation in R is sum of eigen values of $\\sum_R$ which is $trace(\\sum_R)$ \n",
    "- Proportion of variation explained by kth principal component $\\frac{\\lambda_k}{tr(\\sum_R)}$\n",
    "\n",
    "### Granger Causality\n",
    "- To model the directionality of shocks in a system, the authors use a concept called 'Granger Causality' \n",
    "- Time series j is said to 'Granger-Cause' time series i if past values of j contain information helps predict i above and beyond the information contained in past values of i alone. \n",
    "- Let $R_t^i$ and $R_t^j$ be two (Tx1) time series with 0 mean.  Their linear relationship is represented with the following model: \n",
    "\n",
    "$R_t^i = \\gamma_0 + \\gamma_1R_{t-1}^i + \\gamma_2 R_{t-1}^j + \\epsilon_t^i$\n",
    "\n",
    "$R_t^j = \\beta_0 + \\beta_1R_{t-1}^i + \\beta_2 R_{t-1}^j + \\epsilon_t^j$\n",
    "\n",
    "for an observation at time t.  \n",
    "\n",
    "Collecting the terms in a matrix, we see: \n",
    "\n",
    "$Z_t = \\theta_0 + \\theta_1 Z_{t-1} + \\epsilon_t$\n",
    "\n",
    "where $Z_t = \\begin{pmatrix} R_t^i \\\\ R_t^j\\end{pmatrix}$ is a (2T x 1) vector, $\\theta_0 = \\begin{pmatrix}\\gamma_0 \\\\ \\beta_0\\end{pmatrix}$ is a (2T x 1) vector, $\\theta_1 = \\begin{pmatrix} \\gamma_1 & \\gamma_2 \\\\\n",
    "\\beta_1 & \\beta_2 \\end{pmatrix}$ is a (2T x 2T) matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulating a bivariate VAR process\n",
    "from vector_ar.bivar import BiVariateVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<vector_ar.bivar.BiVariateVar at 0x7feae095a550>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from vector_ar.bivar import BiVariateVar\n",
    "bivar = BiVariateVar()\n",
    "x, y = bivar._simulate_var()\n",
    "bivar.fit(x, y, p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[3.00220432, 0.39220357, 0.30608992],\n       [4.83016781, 0.21222938, 0.09957936]])"
     },
     "metadata": {},
     "execution_count": 10
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "correlation matrix\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1.        , 0.80759851],\n       [0.80759851, 1.        ]])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "bivar.theta\n",
    "print(\"correlation matrix\")\n",
    "np.corrcoef((bivar.residuals[:,0], bivar.residuals[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "data = pd.DataFrame(np.concatenate((x[:, np.newaxis], y[:, np.newaxis]), axis=1))\n",
    "v = VAR(data)\n",
    "result = v.fit(1, trend='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  Summary of Regression Results   \n==================================\nModel:                         VAR\nMethod:                        OLS\nDate:           Fri, 14, Aug, 2020\nTime:                     23:40:06\n--------------------------------------------------------------------\nNo. of Equations:         2.00000    BIC:                  -0.970110\nNobs:                     999.000    HQIC:                 -0.988379\nLog likelihood:          -2329.75    FPE:                   0.368034\nAIC:                    -0.999580    Det(Omega_mle):        0.365833\n--------------------------------------------------------------------\nResults for equation 0\n========================================================================\n           coefficient       std. error           t-stat            prob\n------------------------------------------------------------------------\nconst         3.109944         0.231703           13.422           0.000\nL1.0          0.387471         0.041874            9.253           0.000\nL1.1          0.297355         0.051129            5.816           0.000\n========================================================================\n\nResults for equation 1\n========================================================================\n           coefficient       std. error           t-stat            prob\n------------------------------------------------------------------------\nconst         5.009726         0.227217           22.048           0.000\nL1.0          0.204343         0.041063            4.976           0.000\nL1.1          0.085021         0.050139            1.696           0.090\n========================================================================\n\nCorrelation matrix of residuals\n            0         1\n0    1.000000  0.807017\n1    0.807017  1.000000\n\n"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1.        , 0.79996231],\n       [0.79996231, 1.        ]])"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "np.corrcoef((var.residuals[:,0], var.residuals[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = v.select_order(trend='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<class 'statsmodels.iolib.table.SimpleTable'>",
      "text/html": "<table class=\"simpletable\">\n<caption>VAR Order Selection (* highlights the minimums)</caption>\n<tr>\n   <td></td>      <th>AIC</th>         <th>BIC</th>         <th>FPE</th>        <th>HQIC</th>    \n</tr>\n<tr>\n  <th>0</th>  <td>   -0.4281</td>  <td>   -0.4182</td>  <td>    0.6517</td>  <td>   -0.4243</td> \n</tr>\n<tr>\n  <th>1</th>  <td>   -0.9935*</td> <td>   -0.9636*</td> <td>    0.3703*</td> <td>   -0.9821*</td>\n</tr>\n<tr>\n  <th>2</th>  <td>   -0.9904</td>  <td>   -0.9405</td>  <td>    0.3714</td>  <td>   -0.9715</td> \n</tr>\n<tr>\n  <th>3</th>  <td>   -0.9841</td>  <td>   -0.9142</td>  <td>    0.3738</td>  <td>   -0.9575</td> \n</tr>\n<tr>\n  <th>4</th>  <td>   -0.9792</td>  <td>   -0.8894</td>  <td>    0.3756</td>  <td>   -0.9450</td> \n</tr>\n<tr>\n  <th>5</th>  <td>   -0.9752</td>  <td>   -0.8654</td>  <td>    0.3771</td>  <td>   -0.9334</td> \n</tr>\n<tr>\n  <th>6</th>  <td>   -0.9693</td>  <td>   -0.8395</td>  <td>    0.3794</td>  <td>   -0.9199</td> \n</tr>\n<tr>\n  <th>7</th>  <td>   -0.9679</td>  <td>   -0.8182</td>  <td>    0.3799</td>  <td>   -0.9110</td> \n</tr>\n<tr>\n  <th>8</th>  <td>   -0.9614</td>  <td>   -0.7917</td>  <td>    0.3824</td>  <td>   -0.8968</td> \n</tr>\n<tr>\n  <th>9</th>  <td>   -0.9543</td>  <td>   -0.7646</td>  <td>    0.3851</td>  <td>   -0.8821</td> \n</tr>\n<tr>\n  <th>10</th> <td>   -0.9485</td>  <td>   -0.7389</td>  <td>    0.3873</td>  <td>   -0.8688</td> \n</tr>\n<tr>\n  <th>11</th> <td>   -0.9413</td>  <td>   -0.7117</td>  <td>    0.3901</td>  <td>   -0.8540</td> \n</tr>\n<tr>\n  <th>12</th> <td>   -0.9413</td>  <td>   -0.6918</td>  <td>    0.3901</td>  <td>   -0.8464</td> \n</tr>\n<tr>\n  <th>13</th> <td>   -0.9363</td>  <td>   -0.6668</td>  <td>    0.3921</td>  <td>   -0.8338</td> \n</tr>\n<tr>\n  <th>14</th> <td>   -0.9292</td>  <td>   -0.6397</td>  <td>    0.3949</td>  <td>   -0.8190</td> \n</tr>\n<tr>\n  <th>15</th> <td>   -0.9249</td>  <td>   -0.6154</td>  <td>    0.3966</td>  <td>   -0.8072</td> \n</tr>\n<tr>\n  <th>16</th> <td>   -0.9200</td>  <td>   -0.5906</td>  <td>    0.3985</td>  <td>   -0.7947</td> \n</tr>\n<tr>\n  <th>17</th> <td>   -0.9178</td>  <td>   -0.5684</td>  <td>    0.3994</td>  <td>   -0.7848</td> \n</tr>\n<tr>\n  <th>18</th> <td>   -0.9122</td>  <td>   -0.5428</td>  <td>    0.4017</td>  <td>   -0.7717</td> \n</tr>\n<tr>\n  <th>19</th> <td>   -0.9132</td>  <td>   -0.5239</td>  <td>    0.4013</td>  <td>   -0.7651</td> \n</tr>\n<tr>\n  <th>20</th> <td>   -0.9091</td>  <td>   -0.4998</td>  <td>    0.4029</td>  <td>   -0.7534</td> \n</tr>\n<tr>\n  <th>21</th> <td>   -0.9022</td>  <td>   -0.4729</td>  <td>    0.4057</td>  <td>   -0.7389</td> \n</tr>\n</table>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "chk.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = bivar.auto_select(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'min_bic': (1, -0.971484646091933),\n 'bic_results': {(0, -0.3718468862128184),\n  (1, -0.971484646091933),\n  (2, -0.9482981853109385),\n  (3, -0.922363405860413),\n  (4, -0.8978131227858318),\n  (5, -0.8728075628730364)}}"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "d # high agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking garch model\n",
    "from garch.garch import Garch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 0.1\n",
    "gamma = 0.4\n",
    "beta = 0.2\n",
    "w = np.random.standard_normal(10000)\n",
    "a = np.zeros_like(w)\n",
    "var = np.zeros_like(w)\n",
    "\n",
    "for t in range(1, 10000):\n",
    "    var[t] = omega + gamma * a[t-1]**2 + beta * var[t-1]\n",
    "    a[t] = w[t] * np.sqrt(var[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Optimization terminated successfully.\n         Current function value: -0.296176\n         Iterations: 11\n         Function evaluations: 65\n         Gradient evaluations: 13\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<garch.garch.Garch at 0x7f9ea920f048>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "g = Garch(mean=False)\n",
    "g.fit(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.0947502 , 0.36569889, 0.22662725])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "g.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}